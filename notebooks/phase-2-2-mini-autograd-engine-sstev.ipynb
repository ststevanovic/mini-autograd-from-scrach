{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e109c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Optional, Tuple, List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06244d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Tensor:\n",
    "    data: np.ndarray\n",
    "    requires_grad: bool = False\n",
    "\n",
    "    _grad: Optional[np.ndarray] = field(default=None, init=False, repr=False)\n",
    "    _backward: Callable[..., None] = field(default=lambda *args, **kwargs: None,\n",
    "                                       init=False, repr=False)\n",
    "    _prev: Tuple[\"Tensor\", ...] = field(default_factory=tuple, init=False, repr=False)\n",
    "    _op: str = field(default=\"\", init=False, repr=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not isinstance(self.data, np.ndarray):\n",
    "            self.data = np.array(self.data)\n",
    "        if self.data.dtype.kind != \"f\":  # enforce floating types for gradients\n",
    "            self.data = self.data.astype(np.float32)\n",
    "\n",
    "    @property\n",
    "    def grad(self) -> Optional[np.ndarray]:\n",
    "        return self._grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self._grad = None\n",
    "\n",
    "    def _ensure_same_dtype(self, other: \"Tensor\"):\n",
    "        if self.data.dtype != other.data.dtype:\n",
    "            raise TypeError(f\"dtype mismatch: {self.data.dtype} vs {other.data.dtype}\")\n",
    "    \n",
    "    # ----- core ops -----\n",
    "\n",
    "    def __add__(self, other: \"Tensor\") -> \"Tensor\":\n",
    "        self._ensure_same_dtype(other)\n",
    "        out = Tensor(self.data + other.data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "        out._op, out._prev = \"add\", (self, other)\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self._accum_grad(self._broadcast_like(out._grad, self.data.shape))\n",
    "            if other.requires_grad:\n",
    "                other._accum_grad(self._broadcast_like(out._grad, other.data.shape))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other: \"Tensor\") -> \"Tensor\":\n",
    "        self._ensure_same_dtype(other)\n",
    "        out = Tensor(self.data * other.data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "        out._op, out._prev = \"mul\", (self, other)\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self._accum_grad(self._broadcast_like(other.data * out._grad, self.data.shape))\n",
    "            if other.requires_grad:\n",
    "                other._accum_grad(self._broadcast_like(self.data * out._grad, other.data.shape))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __matmul__(self, other: \"Tensor\") -> \"Tensor\":\n",
    "        # matrix multiply: (m,k) @ (k,n) -> (m,n)\n",
    "        self._ensure_same_dtype(other)\n",
    "        out = Tensor(self.data @ other.data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "        out._op, out._prev = \"matmul\", (self, other)\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self._accum_grad(out._grad @ other.data.T)\n",
    "            if other.requires_grad:\n",
    "                other._accum_grad(self.data.T @ out._grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sum(self) -> \"Tensor\":\n",
    "        out = Tensor(np.array(self.data.sum(), dtype=np.float32), requires_grad=self.requires_grad)\n",
    "        out._op, out._prev = \"sum\", (self,)\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self._accum_grad(np.ones_like(self.data) * out._grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def relu(self) -> \"Tensor\":\n",
    "        out = Tensor(np.maximum(self.data, 0), requires_grad=self.requires_grad)\n",
    "        out._op, out._prev = \"relu\", (self,)\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self._accum_grad((self.data > 0).astype(self.data.dtype) * out._grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # ----- autograd driver -----\n",
    "\n",
    "    def backward(self, grad: Optional[np.ndarray] = None):\n",
    "        \"\"\"\n",
    "        Reverse-mode autodiff: seed with dL/d(out).\n",
    "        If tensor is scalar and no grad provided, seed with 1.\n",
    "        \"\"\"\n",
    "        if not self.requires_grad and grad is None:\n",
    "            raise RuntimeError(\"backward on a tensor that does not require grad\")\n",
    "\n",
    "        if grad is None:\n",
    "            if self.data.size != 1:\n",
    "                raise RuntimeError(\"non-scalar backward requires grad seed\")\n",
    "            grad = np.ones_like(self.data)\n",
    "\n",
    "        # topological order (post-order) over the tape\n",
    "        topo: List[Tensor] = []\n",
    "        visited = set()\n",
    "\n",
    "        def build(v: Tensor):\n",
    "            if id(v) in visited:\n",
    "                return\n",
    "            visited.add(id(v))\n",
    "            for p in v._prev:\n",
    "                build(p)\n",
    "            topo.append(v)\n",
    "\n",
    "        build(self)\n",
    "\n",
    "        # seed\n",
    "        self._grad = grad\n",
    "\n",
    "        # reverse pass\n",
    "        for v in reversed(topo):\n",
    "            if v._grad is None:\n",
    "                continue\n",
    "            v._backward() \n",
    "\n",
    "    # ----- helpers -----\n",
    "\n",
    "    def _accum_grad(self, g: np.ndarray):\n",
    "        if self._grad is None:\n",
    "            self._grad = g\n",
    "        else:\n",
    "            self._grad = self._grad + g\n",
    "\n",
    "    def _broadcast_like(self, grad: np.ndarray, target_shape: Tuple[int, ...]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Sum grad over broadcasted dimensions so it matches target_shape.\n",
    "        \"\"\"\n",
    "        # collapse extra leading dims\n",
    "        while grad.ndim > len(target_shape):\n",
    "            grad = grad.sum(axis=0)\n",
    "        # sum over axes where target dim == 1 but grad dim > 1\n",
    "        for i, (gdim, tdim) in enumerate(zip(grad.shape, target_shape)):\n",
    "            if tdim == 1 and gdim > 1:\n",
    "                grad = grad.sum(axis=i, keepdims=True)\n",
    "        return grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
